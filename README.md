# ğŸ‘ï¸ Eye Tracking & Blink-Based Mouse Control

This Python application enables **hands-free control of your computer** using **eye movement for cursor control** and **blinks for clicking**, using only a **webcam**. It leverages **MediaPipe** for face and eye landmark detection, and trains a **regression model** to map your eye position to screen coordinates.

---

## ğŸš€ Features

- ğŸ¯ Real-time **eye tracking** for cursor movement  
- ğŸ‘ï¸â€ğŸ—¨ï¸ Blink detection for **click actions**:
  - **Triple blink** â†’ Left Click
  - **Quadruple blink** â†’ Right Click
- ğŸ§  Machine learning-based cursor prediction using **Ridge Regression** with polynomial features
- ğŸ“ One-time **calibration** to adapt the model to your face and camera setup
- ğŸ“¸ Only requires a **webcam**

---

## ğŸ› ï¸ Requirements

Install required packages using pip:

```bash
pip install opencv-python mediapipe pyautogui scikit-learn joblib numpy
```

---

## ğŸ“¦ Files

- `eye_tracking.py` â€” Main script
- `eye_tracker_model.joblib` â€” Saved model after calibration (auto-generated)
- `README.md` â€” Project documentation

---

## ğŸ§ª First-Time Setup (Calibration Required)

Before tracking can begin, you must **calibrate** the system so it learns how your eye positions map to screen coordinates.

### âœ… Steps:

1. Run the script:
   ```bash
   python eye_tracking.py
   ```
2. If no model is found, calibration will start automatically.
3. **Follow the instructions on screen** â€” move your eyes to the cursor position as it moves.
4. Wait 7 seconds per target location while the system collects data.
5. Once complete, the model is saved automatically.

> âš ï¸ **Important**: Do not move your head during calibration. Only move your eyes!

---

## ğŸ•¹ï¸ Usage After Calibration

After the model is trained:

- Just run:
  ```bash
  python eye_tracking.py
  ```
- The cursor will follow your eye movement.
- Blink 3 times quickly to **left click**, or 4 times to **right click**.

---

## ğŸ“ Optional: Evaluate Model Accuracy

To get feedback on how accurate the model is, you can modify the script to print:

```python
from sklearn.metrics import mean_absolute_error, r2_score

pred = model.predict(X_calib)
print("MAE:", mean_absolute_error(y_calib, pred))
print("R2 Score:", r2_score(y_calib, pred))
```

---

## ğŸ’¡ Tips

- Use in **well-lit** environments for better face and eye detection.
- Ensure the webcam is at **eye level** and your **face remains stable**.
- Re-run calibration if you change your environment or lighting.

---

## ğŸ“„ License

This project is open-source and available under the [MIT License](LICENSE).
